import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.lib.input.*;
import org.apache.hadoop.lib.output.*;

public class UAExample{
	pulbic static class UAMapper extends Mapper<Object, Text, Text, IntWritable>{
		public void map(Object key,Text value, Context context)
			throws IoException, InterruptedException, ClassNotFoundException{			
			String line=value.toString();
			String[] words=line.split(" ");
			context.write(new Text(words[0]), new IntWritable(1));
		}

	public static class UAReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
		public void reduce(Text key, Iterable<IntWritable> values, Context context)
			throws IoException, InterruptedException, ClassNotFoundException{
			int sum=0;
			for(IntWritable v:values){
				sum+=v.get();}
			sum*=2;
			context.write( key, new Text("The value is "+sum));
		}

	}

public static void main(String[] args)
	throws IoException, InterruptedException, ClassNotFoundException
	{
	Job job=new Job();
	job.setMapperClass();
	job.setReducerClass();
	//reducer
	job.setOutputKeyClass();
	job.setOutputValueClass();
	//mapper
	job.setMapOutputKeyClass(Text.class);
	job.setMapOutputValueClass(IntWritable.class);

	FileInputFormat.addInputPath(job, new Path (args[0]) );
	FileOutputFromat.setOutputPath(job, new Path(args[1]));
	System.exit(job.waitForCompilation(true) ? 0 : 1 ); 
	}